# Simple Language Model
![alt text](image.png)

A minimal feedâ€‘forward neural language model built with **PyTorch**. It takes a fixedâ€‘length context window of 5 BERT token IDs and predicts the next token. Using a tinyshakespeare.txt dataset

---

## âœ¨Â Key points

* **Tiny architecture**â€ƒ`Embedding â†’ Average â†’ FC â†’ ReLU â†’ FC` (â‰ˆÂ 300Â k trainable parameters)
* **CSV data pipeline**â€ƒcontext + target stored as rows: `tok1,tok2,tok3,tok4,tok5,target`
* **Trainer class**â€ƒhandles dataloader, loss, optimizer, checkpointing
* Works with any text after tokenising by *bertâ€‘baseâ€‘uncased* tokenizer

---

## ðŸ”§Â Requirements

| Package | Version |
|---------|---------|
| Python  | â‰¥Â 3.9   |
| PyTorch | â‰¥Â 1.13 or 2.x |
| transformers | â‰¥Â 4.40 |

Install dependencies:
```bash
pip install torch transformers tqdm
